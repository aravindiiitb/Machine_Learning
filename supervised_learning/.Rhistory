source('~/personal/linear_regression.R', echo=TRUE)
source('~/personal/linear_regression.R', echo=TRUE)
head(rawData, 10)
linModel <- lm(y~x, data = rawData)
rawData=read.csv("linear.csv", header=T)
# Set path to Desktop
setwd("~/personal/r-practicing")
rawData=read.csv("linear.csv", header=T)
# Show first n entries of data.frame, notice NA values
head(rawData, 10)
linModel <- lm(y~x, data = rawData)
linModel
# Show attributes of linModel
attributes(linModel)
linModel$coefficients
linModel$model
# To show what happens with na.action, "omit" since data has NA
linModel$na.action
data.frame(x = 3)
# Predicting New Value based on our model
predict(linModel, data.frame(x = 3))
setwd("~/personal/iris_data_analysis")
setwd("~/personal/r-practicing/iris_data_analysis")
data = read.csv(file="data.csv",header = T, sep = ",")
data
summary(data)
data = read.csv(file="data.csv",header = TRUE, sep = ",")
data
summary(data)
data[0]
type(data)
typeof(data)
typeof(data$X5.1)
data$X5.1[0]
data$X5.1[1]
data = read.csv(file="data.csv",headers = TRUE, sep = ",")
data = read.csv(file="data.csv",header = TRUE, sep = ",")
data
typeof(data
)
data$sepal_length
data[1:3,]
data[,1:3]
data[1:3,1:3]
plot(data[,1:3])
plot(data[,:-1])
data[1:3,:-1]
data[1:3,:1]
data[1:3,:3]
plot(data[,1:4])
# data
plot(data[,1:4], type = "0")
# data
plot(data[,1:4], type = "o")
# data
plot(data[,1:4])
# data
plot(data[,1:1])
v <- c(7,12,28,3,41)
# Give the chart file a name.
png(file = "line_chart.jpg")
# Plot the bar chart.
plot(v,type = "o")
# Save the file.
dev.off()
typeof(v)
# Give the chart file a name.
png(file = "line_chart.jpg")
# Plot the bar chart.
plot(data[,1:1],type = "o")
# Save the file.
dev.off()
summary(data)
png(file = "histogram.png")
hist(data[,1:1],xlab = "Weight",col = "yellow",border = "blue")
# Save the file.
dev.off()
hist(data)
hist(data[,1:4])
hist(data[,1:1],xlab = "Weight",col = "yellow",border = "blue")
hist(data[,1:1],xlab = "Weight",col = "yellow",border = "blue")
hist(data[,1:1],xlab = "Weight",col = "yellow",border = "blue")
colnames(data)
plot(data[,1:4])
require(caTools)
install.packages("caTools")
require(caTools)
# Read the data
data = read.csv(file="data.csv",header = TRUE, sep = ",")
data
# Print the statstics of the data
summary(data)
# Train/Test split of the data
set.seed(101)
sample = sample.split(data$class, SplitRatio = .8)
train = subset(data, sample == TRUE)
train
train
test = subset(data, sample == FALSE)
test = subset(data, sample == FALSE, drop = 5)
test = subset(data, sample == FALSE, drop = 4)
test = subset(data, sample == FALSE, drop(4))
sample = sample.split(data$class, SplitRatio = .8)
train = subset(data, sample == TRUE)
test = subset(data, sample == FALSE)
head(train)
# Print the statstics of the data
summary(data)
head(test)
head(test[,1:4])
head(test[,1:4])
# install.packages("caTools")
library("nnet")
model = multinom(class ~ sepal_length + sepal_width + petal_length + petal_width, data = train)
summary(model)
library(caret)
library("caret")
install.packages("caret")
library("caret")
a = data("iris")
head(a)
a = data(iris)
head(a)
a
# Define train control for k fold cross validation
train_control <- trainControl(method="cv", number=10)
train_control
# Multinomial Logistic Regression
model = multinom(class ~ ., data = data, trControl=train_control)
summary(model)
# Multinomial Logistic Regression
model = multinom(class ~ ., data = data, trControl=train_control)
summary(model)
train_control
set.seed(123)
ind_train <- lapply(split(seq(1:nrow(data)), data$class), function(x) sample(x, floor(.6*length(x))))
ind_test <- mapply(function(x,y) setdiff(x,y), x = split(seq(1:nrow(data)), data$class), y = ind_train)
View(ind_test)
View(ind_train)
View(ind_test)
# Multinomial Logistic Regression
model = multinom(class ~ ., data = ind_train)
View(ind_train)
typeof(ind_train)
data[unlist(ind_train),]
len(data[unlist(ind_train),])
length(data[unlist(ind_train),])
# Multinomial Logistic Regression
model = multinom(class ~ ., data = data[unlist(ind_train),])
summary(model)
data[unlist(ind_test),]
data[unlist(ind_test),][,1:4]
output = predict(model, data = data[unlist(ind_test),][,1:4])
output
data[unlist(ind_test),][,5:5]
data[unlist(ind_test),][,5:5].length()
output
output$predicted = output
output$actuals = data[unlist(ind_test),][,5:5]
head(output)
View(output)
final <- rbind(output, data[unlist(ind_test),][,5:5])
final
drop(final)
output = predict(model, data = data[unlist(ind_test),][,1:4])
final <- rbind(output, data[unlist(ind_test),][,5:5])
final
colnames(final)
head(final)
output = predict(model, data = data[unlist(ind_test),][,1:4])
cbind(output, actuals=data[unlist(ind_test),][,5:5])
cbind(output, vals=ifelse()
;
colnames(output)
a = cbind(output, actuals=data[unlist(ind_test),][,5:5])
a
head(a)
a <- cbind(a, val=ifelse(output == actuals, 1, 0))
a <- cbind(a, val=ifelse(a$output == a$actuals, 1, 0))
len(a)
size(a)
length(a)
# Reference: https://github.com/bensadeghi/R-demos/blob/master/CRAN-R/2_Iris_Predictive_Analysis.R
data("iris")
# Reference: https://github.com/bensadeghi/R-demos/blob/master/CRAN-R/2_Iris_Predictive_Analysis.R
data(iris)
head(iris)
# Randomly sample iris data to generate training and testing sets
ind <- sample(2, nrow(iris), replace = TRUE, prob = c(0.7, 0.3))
train_data <- iris[ind == 1,]
test_data <- iris[ind == 2,]
ind
train_data
nrow(iris)
# Create the decision tree. The tree is attempting to predict Species based upon sepal length, width and petal length and width
install.packages("party", dependencies = TRUE)
library(party)
head(iris)
my_formula <- Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
iris_ctree <- ctree(my_formula, data = train_data)
# Print the tree
print(iris_ctree)
summary(iris_ctree)
# Plot the tree
plot(iris_ctree)
# Plot a simplified view of the tree
plot(iris_ctree, type = "simple")
# Check the predictions with the training set (confusion matrix)
preds <- predict(iris_ctree)
preds
table(preds, train_data$Species)
# Check the predictions with the test set
preds <- predict(iris_ctree, newdata = test_data)
cm <- table(preds, test_data$Species)
cm
# Compute accuracy of test predictions
sum(diag(cm)) / sum(cm)
# my_formula <- Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
model = multinom(my_formula, data = train_data)
# Print the tree
print(model)
# Plot the tree
plot(model)
# Check the predictions with the training set (confusion matrix)
preds <- predict(iris_ctree)
preds
table(preds, train_data$Species)
# Check the predictions with the training set (confusion matrix)
preds <- predict(iris_ctree)
# Check the predictions with the training set (confusion matrix)
preds <- predict(model)
cm1 <- table(preds, train_data$Species)
cm1
sum(diag(cm1)) / sum(cm1)
# Check the predictions with the test set
preds <- predict(model, newdata = test_data)
cm <- table(preds, test_data$Species)
cm
# Compute accuracy of test predictions
sum(diag(cm)) / sum(cm)
# original example from Digg Data website (Takashi J. OZAKI, Ph. D.)
# http://diggdata.in/post/58333540883/k-fold-cross-validation-in-r
library(plyr)
library(randomForest)
data <- iris
# in this cross validation example, we use the iris data set to
# predict the Sepal Length from the other variables in the dataset
# with the random forest model
k = 5 #Folds
# sample from 1 to k, nrow times (the number of observations in the data)
data$id <- sample(1:k, nrow(data), replace = TRUE)
list <- 1:k
# prediction and testset data frames that we add to with each iteration over
# the folds
prediction <- data.frame()
testsetCopy <- data.frame()
#Creating a progress bar to know the status of CV
progress.bar <- create_progress_bar("text")
progress.bar$init(k)
for (i in 1:k){
# remove rows with id i from dataframe to create training set
# select rows with id i to create test set
trainingset <- subset(data, id %in% list[-i])
testset <- subset(data, id %in% c(i))
# run a random forest model
mymodel <- randomForest(trainingset$Sepal.Length ~ ., data = trainingset, ntree = 100)
# remove response column 1, Sepal.Length
temp <- as.data.frame(predict(mymodel, testset[,-1]))
# append this iteration's predictions to the end of the prediction data frame
prediction <- rbind(prediction, temp)
# append this iteration's test set to the test set copy data frame
# keep only the Sepal Length Column
testsetCopy <- rbind(testsetCopy, as.data.frame(testset[,1]))
progress.bar$step()
}
# add predictions and actual Sepal Length values
result <- cbind(prediction, testsetCopy[, 1])
names(result) <- c("Predicted", "Actual")
result$Difference <- abs(result$Actual - result$Predicted)
# As an example use Mean Absolute Error as Evalution
summary(result$Difference)
install.packages("randomForest")
# original example from Digg Data website (Takashi J. OZAKI, Ph. D.)
# http://diggdata.in/post/58333540883/k-fold-cross-validation-in-r
library(plyr)
library(randomForest)
data <- iris
# in this cross validation example, we use the iris data set to
# predict the Sepal Length from the other variables in the dataset
# with the random forest model
k = 5 #Folds
# sample from 1 to k, nrow times (the number of observations in the data)
data$id <- sample(1:k, nrow(data), replace = TRUE)
list <- 1:k
# prediction and testset data frames that we add to with each iteration over
# the folds
prediction <- data.frame()
testsetCopy <- data.frame()
#Creating a progress bar to know the status of CV
progress.bar <- create_progress_bar("text")
progress.bar$init(k)
for (i in 1:k){
# remove rows with id i from dataframe to create training set
# select rows with id i to create test set
trainingset <- subset(data, id %in% list[-i])
testset <- subset(data, id %in% c(i))
# run a random forest model
mymodel <- randomForest(trainingset$Sepal.Length ~ ., data = trainingset, ntree = 100)
# remove response column 1, Sepal.Length
temp <- as.data.frame(predict(mymodel, testset[,-1]))
# append this iteration's predictions to the end of the prediction data frame
prediction <- rbind(prediction, temp)
# append this iteration's test set to the test set copy data frame
# keep only the Sepal Length Column
testsetCopy <- rbind(testsetCopy, as.data.frame(testset[,1]))
progress.bar$step()
}
# add predictions and actual Sepal Length values
result <- cbind(prediction, testsetCopy[, 1])
names(result) <- c("Predicted", "Actual")
result$Difference <- abs(result$Actual - result$Predicted)
# As an example use Mean Absolute Error as Evalution
summary(result$Difference)
data$id
1:5
library(plyr)
library(randomForest)
colnames(testset)
